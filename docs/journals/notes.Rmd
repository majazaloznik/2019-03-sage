# R basics 

So, next I want to go through some R basics with you, explain some terminology that is required to get to grips with R. If you are used to working with more point and click types of software, for example using the menus in SPSS, then get ready for some coding :)

## calculations
But let's start easy, with calculations. R can be used directly as a calculator

So let's try a few easy calculations. 
You can use brackets as well, the usual rules apply. Now if I accidentally forget to close a bracket, this happens 

If I on the other hand forget to open a bracket, we get the following error. 


# variable assignment

The next thing we will want to do is to represent numbers with variables. Once we assign a value to a variable, we are then able to call in at a later date and reuse it in that way. You will also notice that the list of variables available in our workspace can be seen here in the Environment tab. 

# data types/data structures

So far we have been looking at scalars, so single value variables, single numbers. But of course R can treat much more complicated objects than single numbers. 

## vectors

A one dimensional collection of scalars is known as a vector. In order to create a vector we need to use the function `c()` which combines together elements into a vector. We will discuss functions in a bit more detail in a minute. 

So let's try to create a vector. There, you can also see it in the workspace.
We can do math with vectors, and we can do it very efficiently, this is where R really shines. Let's see what happens when we multiply our vector with 10. I

Let's do another vector quick, same lenght. Now we can try to see what happens when we add these two vectors together. You may have also noticed something else in the environment pane, namely the word num here, it's telling us these vectors are numerical. This are not the only data types, we also have string data and logical data. let's have a look: 

### string data
Actually, you know what, before I continue, I've forgotten something. This script file is starting to look a bit messy right now, and I might get confused next time I look at this not being clear what I was trying to do. So let me first go back and add some comments to make it clear. The way you add comments in R is by starting the line with a hash symbol. Anything written after the hash gets ignored when R computes the code. So let's clean this up a little bit.
OK, now let's write a string vector

### logical data
finally, let's also write a logical vector, this is what a logical vector looks like. 

You will note that vectors can only have elements of a single type, if we try to mix data types, then R will coerce them to the lowest commom denominator, e.g. 
r scripts

## functions

So we've already introduced one function, which was the `c()` funciton combining elements into a vector. Let's have a look at some more, since funcitons really are the working horse of R. There are essentially three sources of functions. Some are functions that are automatically loaded when you load R. These are the most commonly used functions that you want to keep handy. 

For example we could want to calculate the average value in vector one. we could of course type out the whole calculation. But that is slow and error prone.  Althernatively, we could use the sum() function and divide the sum by five. Actually, this dividing it by five is again manual and therefore error prone, we can instead use the function lenght() to get the length of the vector, that means the number of elements in it 

But of course calculating the average is such a common thing that a function exists for that, it is called mean().

So I've now already shown you three new functions, all of which required just one argument, the vector, in order to be executed. But functions can also have more than one argument, for example the function rnorm() which generates random numbers from the normal distribution. We can have a look at the help file for that. So this is what we are looking for, rnorm takes three arguments, n, mean and sd. So we can see that only n is compulsory, since the other arguments have defaults, which will be used if we don't select different ones. 

So I said before there are three sources of functions, one are these base functions that are immediately available, a second one are user defined functions, meaning i can write a function, for example we can write a function to throw a dice. this isn't super useful, but what we can also do is add an argument to tell us how many sides this dice has. 

## functions from packages
the final source of functions is from packages. the very first time you want to use a package, you have to download and install it, you do this with the `install.packages()` function. every time after than, when you want to use funcitons or data from a package you load it using the `library()` function.. for example we can load the xx package, which we will be using in the regression demo part of this webinar. 




## data.structures - data.frames. 

Now at this point you might still be missing one important thing: if you are used to working with most other statistical software you might be wondering: but where are the tables? 

In R the standard type of table is called a data.frame. A data.frame is essentially a collection of vectors of the same lenght. Each vector is therefore a variable, so you can see now why it also makes sense that they can only be of one type. And of course it makes sense that they all have to be the same length, that is the number of rows in our data.frame. 

There are several ways to get a dataframe, most often you will actually import it from a csv file or any other type of file. But you can also create one from scratch, by combining together a set of vectors that fit the bill. So let's use the vectors we created before to create a data.frame. 

I'll first add an additional vector, to make it a bit more interesting. 

OK, so combining them all together we get the following. Now I actually want to clean this up a little bit. For example the names of the variables are not informative at all, so we should change that. And also I think the order would make more sense this way, let's see how that looks like now. Excellent. Now of course we want to save this. 

Now you see my.df has become available here in the workspace and if we click on it we get the Rstudio data viewer. This will seem familiar to anyone used to excel or spss, and that is kind of the reason for it being here really. But there are some importand differences. You cannot change the data in the data viewer. This is in line with principles of reproducibility, making it difficult to make changes to the data, you have to make changes programatically, not by clickin only. 

OK, now that we have a data.frame, let's do a few quick plots while we're at it. So once the vectors are in the dataframe, the names of the columns are not objects in our environments, so year.born doesn't exist as an object outside of the data.frame. So in order to access it directly we use the dollar sign: `my.df$rpc`

Now we can plot the values of the rock paper scissors score simply like this, where the x axis is simply the index of each case, so the row number. This is just to demonstrate to you how simply we can change the type of chart, or colour, or type of symbol. 

We can also do a histogram of the rps scores.

Of course a scatter plot is probably a more interesting way to look at the data, so let's try that. 


# R regression

[datacamp](https://www.datacamp.com/community/tutorials/linear-regression-R)

So we will finish this demonstration off with a look at ordinary least squares regression, since this is a common method of modelling a continuous dependent variable. 

In real life you will have to deal with a lot of what we call janitor work, getting your data ready for analysis, cleaning, adjusting, labelling et cetera. We will skip this part here and work with a nice and tidy data set from gapminder. 

So let's have a lookd at the data first. We load it using library(gapminder). We can have a quick look at the table using the head funcition, which shows us the first six rows of the table. If we want to see the table in the viewer, you will notice that it is not in the global environment, but if you look at this menu here you will find the package environment where it is.
So we've got data for countries, life expectancy, population size, gdp per capita, for the following years. So in this analysis we will treat life expectancy as the dependent variable, and the year and gdp per capita as independent variables. 

Let's first look at the individual correlations between the variables. This is life expectancy by year. In order to get a better view of the data, we can actually add some jitter to the year varible, because the points are all stacked together now making it difficult to see the distribution.

And this is life expectancy by gdp per capita. 

So in order to fit a linear regression model in R we use the function  lm() which stands for linear model, and is in loaded with the basic R packages. The lm() function can take numerous arguments, all of which are explained in the help, but for now we will only use two: the formula and the data. 

So we will first regress the life expectancy on the year. We will save the regression model into a new object calling it reg1 and inspect it using summary(). Now let's just quickly unpack this. At the top we have a reminder of the function call. 

So the first thing we probably want to look at are the coefficients, and they are here: the intercept is minus 585 and the coefficient for the year is point 3. We can extract them quite easily from the reg1 object, which is a list, by using the dollar sign like so. 
Now there is actually a nice plotting function in R called abline, which will plot a line using slope and intercept values, so we can try that. So this simple model is saying that for every year increase the live expectancy goes up by about a third of a year on average. The standard error is very low here, so the actual average life expectancy is .016 of a year off the model's predictions. Then we have the t values for the coefficients, which are very high in this case and consequently the p-values for both coefficients are highly significant. Meaning that if there were no relationship between the two variables we would be highly unlikely to have observed this data. 

What else do we have, we have a summarey of the distribution of the residuals here. We can see they don't look perfectly symmetrical, so we could investigate that a bit further, but we're not going to now. 

So how good a model is this for explainign the variation in life expectancy? Well, we've got the R squared value down here, and it's about 19 percent, so not ideal. So roughtly 19 percent of the variance in life expectancy is explained by the year in which it is measured. Now R squared can only increase as you add more predictors, so for multiple regressions you would want to look at the adjusted r squared instead, which we will do in a second. 

So now let's try gdp per capita as well, simple model looks like this. 

Of course you can probably quickly think of what would imporve this model substantially, which is to take the logarithm of the gdp, and linearise the relationship. So let's try that. Lovely

Now let's try a multiple regression model, so add both predictors into our model and see how that affects it: You do that by using the plus operator 

interpretation  of the data

social science

more soft bits, 

make it a bit more human relatable. 


## Multiple regression with interaction term
```{r, echo = FALSE}
suppressMessages(library(effects))
interaction <- effect("log(gdpPercap) * continent", reg6)
plot(interaction, style = "line", multiline = TRUE)
```

# Notes on andrea

Exploring Rstudio video 
* types in `r` in the console as a demonstration - of nothing!?
* says you have to run ctrl+enter or command+enter, which isn't true, enter is enough
* in customizing rstudio text she says you have to place your cursor at the start or end of the code, also not true, just has to be in the command anywhere
* in video one of module two, the R language simplified, she goes straight into functions, and also does a crap job at explaining what they are..
* the function(objectname) bit doesn't make any sense
* omg, the lists video... so stupid, completely unnecesary, then spends too much time talking about spaces before assignment operators, and in the end ends up being inconsistent about it.. even though data.frames are lists, you don't really need to know that to "understand them" as she says. 